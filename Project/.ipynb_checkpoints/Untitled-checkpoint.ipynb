{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc37eec-0657-4899-8b26-9ebfd9af7196",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        *** Project: Winter 2025 ***\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>I. Team members</h2>\n",
    "<b>\n",
    "    \n",
    "- Minh Le Nguyen\n",
    "- Liam Knapp\n",
    "- Gautam Singh\n",
    "- Gleb Ignatov\n",
    "\n",
    "</b>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f93877-76bf-4961-aa56-ac71aad42a28",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d984a-de7e-4e73-9e2c-6422b63ac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7fccf-a3a9-4536-a31b-2267562affbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore, skew, kurtosis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c5190-6c95-4d6b-91bb-4c3068b83008",
   "metadata": {},
   "source": [
    "### 1. Load Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238e2b3-3b18-400a-b6fa-b7902ded42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset_dataframe = pd.read_csv(\"./Bigdataset/BreakData/part_1.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd324fb-2841-4a2d-bfe8-c6879be44420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(large_dataset_dataframe.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8ca48-1466-4ddd-ab36-23897b0a7b94",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c40ad-0ac1-40e5-82d2-a368d6a87713",
   "metadata": {},
   "source": [
    "#### a. Large Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28914280-c731-4f3a-8407-ecb8d1504e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows inside the dataset: \", large_dataset_dataframe.shape[0])\n",
    "print(\"Number of columns inside the dataset: \", large_dataset_dataframe.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25baee6-0f8e-4954-92ff-5694f15a5be2",
   "metadata": {},
   "source": [
    "#### b. The Columns Names and Dtypes inside Large Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cb45c-da4a-4246-b1d5-5aed1032ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The columns inside the large dataframe\\n\")\n",
    "columns_list = large_dataset_dataframe.columns.to_list()\n",
    "dtypes_list = large_dataset_dataframe.dtypes.to_list()\n",
    "print(f\"{'Column Name':<25} | {'Data Type'}\")\n",
    "print(\"-\" * 40)\n",
    "for col, dtype in zip(columns_list, dtypes_list):\n",
    "    print(f\"{col:<25} | {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006567a-c75e-4408-a9ca-9a98352fd0b0",
   "metadata": {},
   "source": [
    "#### c. Summarize Data Explore inside Large Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a889410-1dd7-4a4d-a841-e9ff3345beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarize_explore():\n",
    "    def __init__(self, processing_file_path):\n",
    "        self.raw_df = pd.read_csv(\"./Bigdataset/BreakData/part_1.csv\", encoding='utf-8')\n",
    "\n",
    "    def print_dataframe_metadata(self):\n",
    "        \"\"\"\n",
    "        Prints metadata information about a Pandas DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        raw_df (pd.DataFrame): The DataFrame for which metadata is to be printed.\n",
    "        \"\"\"\n",
    "        print(\"DataFrame Metadata\\n\")\n",
    "\n",
    "        print(\"\\nSummary Statistics:\\n\")\n",
    "        print(self.raw_df.describe(include='all'))\n",
    "        print(\"\\n================================================================\\n\")\n",
    "        \n",
    "        print(\"\\nShape:\\n\", self.raw_df.shape)\n",
    "        print(\"\\n================================================================\\n\")\n",
    "        \n",
    "        print(\"\\nMissing Values:\\n\")\n",
    "        print(self.raw_df.isnull().sum())\n",
    "        print(\"\\n================================================================\\n\")\n",
    "        \n",
    "        print(\"\\nList of Columns:\\n\")\n",
    "        print(self.raw_df.columns.to_list())\n",
    "        print(\"\\n================================================================\\n\")\n",
    "\n",
    "        print(\"\\nColumns and Data Types:\\n\")\n",
    "        print(self.raw_df.dtypes)\n",
    "        print(\"\\n================================================================\\n\")\n",
    "                \n",
    "        print(\"\\nUnique Values per Column:\\n\")\n",
    "        print(self.raw_df.nunique())\n",
    "        print(\"\\n================================================================\\n\")\n",
    "\n",
    "        print(\"\\nUnique Values per Column:\\n\")\n",
    "        for col in self.raw_df.columns:\n",
    "            print(self.raw_df[col].unique())\n",
    "            print(\"\\n================================================================\\n\")\n",
    "        \n",
    "        print(\"\\nFirst 5 Rows:\\n\")\n",
    "        print(self.raw_df.head())\n",
    "        print(\"\\n================================================================\\n\")\n",
    "\n",
    "\n",
    "    def checking_missing_value(self, input_col_obj, mode=\"single\"):\n",
    "        if mode == \"single\" and isinstance(input_col_obj, str):\n",
    "            df_missing_values_dict = dict()\n",
    "            df_missing_values_dict[input_col_obj] = self.raw_df[input_col_obj].isnull().sum()\n",
    "            missing_values_df = pd.DataFrame(\n",
    "                list(df_missing_values_dict.items()),\n",
    "                columns=[\"Column Name\", \"Number of missing values\"]\n",
    "            )\n",
    "            return missing_values_df\n",
    "        elif mode == \"multiple\" and isinstance(input_col_obj, list):\n",
    "            df_missing_values_dict = dict()\n",
    "            for col in input_col_obj:\n",
    "                df_missing_values_dict[col] = self.raw_df[col].isnull().sum()\n",
    "            \n",
    "            missing_values_df = pd.DataFrame(\n",
    "                list(df_missing_values_dict.items()),\n",
    "                columns=[\"Column Name\", \"Number of missing values\"]\n",
    "            )\n",
    "            return missing_values_df\n",
    "        else:\n",
    "            return \"Please check the function again\"\n",
    "\n",
    "    def distribution_shape_analytics(self, df_input, column_input):\n",
    "        df_process_dis = df_input.dropna().copy()\n",
    "        # Distribution Shape\n",
    "        skewness_temp = skew(df_process_dis[column_input])\n",
    "        kurtosis_temp = kurtosis(df_process_dis[column_input])\n",
    "        print(\"Distribution Shape Analytics for AverageTemperature:\")\n",
    "        print(f\"Distribution Shape - Skewness: {round(skewness_temp,2)}, Kurtosis: {round(kurtosis_temp,2)}\")\n",
    "        if skewness_temp > 0:\n",
    "            print(\"The distribution is positively skewed, indicating a longer tail on the right.\")\n",
    "        elif skewness_temp < 0:\n",
    "            print(\"The distribution is negatively skewed, indicating a longer tail on the left.\")\n",
    "        else:\n",
    "            print(\"The distribution is symmetric.\")\n",
    "        \n",
    "        if kurtosis_temp > 0:\n",
    "            print(\"The distribution has heavier tails and a sharper peak than a normal distribution (leptokurtic).\")\n",
    "        elif kurtosis_temp < 0:\n",
    "            print(\"The distribution has lighter tails and a flatter peak than a normal distribution (platykurtic).\")\n",
    "        else:\n",
    "            print(\"The distribution has a kurtosis similar to a normal distribution (mesokurtic).\")\n",
    "        print(\"\\n\")\n",
    "        return skewness_temp, kurtosis_temp\n",
    "    \n",
    "    def fill_missing_values(self, column_name, fill_value):\n",
    "        self.raw_df[column_name] = self.raw_df[column_name].fillna(fill_value)\n",
    "        return self.raw_df\n",
    "        \n",
    "    def plot_histogram(self, df_input, column_name, default_bin=10):\n",
    "        df_processed = df_input.dropna().copy()\n",
    "        column_data = df_processed[column_name].dropna()\n",
    "        iqr = np.percentile(column_data, 75) - np.percentile(column_data, 25)\n",
    "        bin_width_fd = 2 * iqr / (len(column_data) ** (1 / 3))\n",
    "        if bin_width_fd > 0:\n",
    "            n_bins_fd = int(np.ceil((column_data.max() - column_data.min()) / bin_width_fd))\n",
    "        else:\n",
    "            n_bins_fd = default_bin\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(column_data, bins=n_bins_fd, kde=True, color=\"blue\", alpha=0.6)\n",
    "        plt.title(f'Histogram of {column_name}')\n",
    "        plt.xlabel(column_name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    def detect_outlier(self, column_input):\n",
    "        df_outliers_detect = self.raw_df.dropna().copy()\n",
    "        df_outliers_detect['Z_Score'] = zscore(df_outliers_detect[column_input])\n",
    "        z_outliers = df_outliers_detect[abs(df_outliers_detect['Z_Score']) > 3]\n",
    "        num_z_outliers = len(z_outliers)\n",
    "        Q1 = df_outliers_detect[column_input].quantile(0.25)\n",
    "        Q3 = df_outliers_detect[column_input].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        iqr_outliers = df_outliers_detect[(df_outliers_detect[column_input] < lower_bound) | \n",
    "                                        (df_outliers_detect[column_input] > upper_bound)]\n",
    "        num_iqr_outliers = len(iqr_outliers)\n",
    "        df_outliers_detect['IQR_Outlier'] = (df_outliers_detect[column_input] < lower_bound) | (df_outliers_detect[column_input] > upper_bound)\n",
    "        print(f\"Number of outliers detected by Z-score method: {num_z_outliers}\")\n",
    "        print(f\"Number of outliers detected by IQR method: {num_iqr_outliers}\")\n",
    "        return num_z_outliers, num_iqr_outliers\n",
    "    \n",
    "    def scale_numerical_variables(self, df_input, column_name, method=\"Min-Max\"):\n",
    "        df_processed = df_input.dropna().copy()\n",
    "        if method == \"Min-Max\":\n",
    "            scaler = MinMaxScaler()\n",
    "            df_processed[column_name] = scaler.fit_transform(df_processed[[column_name]])\n",
    "            return df_processed\n",
    "        elif method == \"Standardization\":\n",
    "            scaler = StandardScaler()\n",
    "            df_processed[column_name] = scaler.fit_transform(df_processed[[column_name]])\n",
    "            return df_processed\n",
    "        elif method == \"Robust\":\n",
    "            # RobustScaler uses median and IQR (interquartile range) to scale and making it robust to outliers\n",
    "            scaler = RobustScaler()\n",
    "            df_processed[column_name] = scaler.fit_transform(df_processed[[column_name]])\n",
    "            return df_processed\n",
    "        elif method == \"Log\":\n",
    "            # log(1 + x) to avoid log(0) use logarithmic transformation for normalizing skewed data\n",
    "            df_processed[column_name] = np.log1p(df_processed[column_name])\n",
    "            return df_processed\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaling method. Choose from ['Min-Max', 'Standardization', 'Robust', 'Log']\")\n",
    "\n",
    "    def convert_categorical_variables(self, df_input):\n",
    "        df_processed = df_input['userId'].dropna().copy()\n",
    "        df_input['churn'] = df_input['page'].apply(lambda x: 1 if x == \"Cancellation Confirmation\" else 0)\n",
    "        return df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bee66-63ce-4de0-a011-b673e5ed7ac6",
   "metadata": {},
   "source": [
    "#### d. Print Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d327988-a440-421b-8552-800b4fbab9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_explore_ = summarize_explore(\"./Bigdataset/BreakData/part_1.csv\") \n",
    "df_large_dataset_raw = summarize_explore_.raw_df\n",
    "column_name = df_large_dataset_raw.columns.to_list()\n",
    "summarize_explore_.print_dataframe_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadc479-07b7-4b9b-aa00-3085e4cbdaea",
   "metadata": {},
   "source": [
    "#### e. Check Missing Values inside Columns inside Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfd7a4-a23c-4d89-abc3-2127a174f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_explore_.checking_missing_value(column_name, mode=\"multiple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42560349-3b95-41f3-b35b-89dab1ad516c",
   "metadata": {},
   "source": [
    "#### f. Define the Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c101d6f-b205-4cf1-b002-f653285db930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_dataset_churn_processed = summarize_explore_.convert_categorical_variables(df_large_dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa65bc2-0642-4adc-aaff-4e1ec8c8b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_large_df_explore = df_large_dataset_churn_processed.copy()\n",
    "list_columns = visualize_large_df_explore.drop(\"userId\", axis=1).select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in list_columns:\n",
    "    summarize_explore_.plot_histogram(visualize_large_df_explore, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3eca09-dcda-424f-9747-41cf540eac7c",
   "metadata": {},
   "source": [
    "#### g, Checking Non Missing UserId and Churn Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1f662-966f-4d59-bc46-179515602892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProportion (percentage) of each churn value:\")\n",
    "df_large_dataset_non_missing = df_large_dataset_churn_processed.filter(df_large_dataset_churn_processed[\"userId\"] != \"\")\n",
    "print(len(df_large_dataset_non_missing))\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(df_large_dataset_churn_processed['churn'].value_counts(normalize=True))\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"Count of each churn value:\")\n",
    "print(df_large_dataset_churn_processed['churn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196120f8-cacc-4836-89bc-f1ac83995501",
   "metadata": {},
   "source": [
    "#### h, Remove duplicates based on 'userId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68213ed-bec4-4796-90f7-6b3f30ce3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_dataset_churn_processed_non_missing = df_large_dataset_churn_processed[df_large_dataset_churn_processed[\"userId\"].notna() & (df_large_dataset_churn_processed[\"userId\"] != \"\")]\n",
    "df_large_dataset_churn_processed_non_missing_non_duplicated = df_large_dataset_churn_processed_non_missing.drop_duplicates()\n",
    "print(\"Number of records after processed removed missing and duplicated\", df_large_dataset_churn_processed_non_missing_non_duplicated.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f21af7-6285-4788-abae-5258a76fc4ee",
   "metadata": {},
   "source": [
    "### 3. Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053bea1a-d766-4d9f-bd8d-54247a498e52",
   "metadata": {},
   "source": [
    "#### A, Calculate average churn rate by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e23962-f677-4965-91f5-415a74a57fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stat_large_df = df_large_dataset_churn_processed_non_missing_non_duplicated[['gender', 'churn']]\n",
    "avg_churn_by_gender = gender_stat_large_df.groupby('gender')['churn'].mean() * 100\n",
    "\n",
    "print('The avg churn rate of females is:', avg_churn_by_gender.get('F', 'N/A'))\n",
    "print('The avg churn rate of males is:', avg_churn_by_gender.get('M', 'N/A'))\n",
    "\n",
    "avg_churn_by_gender.plot(kind='bar')\n",
    "plt.ylabel('Average Churn Rate (%)')\n",
    "plt.title('Average Churn Rate by Gender')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3936bce-571d-4d3d-9254-3f06e4ff410b",
   "metadata": {},
   "source": [
    "#### B, Calculate average churn rate by artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7be07-e07e-4092-b028-446b9c080b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_stat_large_df = df_large_dataset_churn_processed_non_missing_non_duplicated[['artist', 'churn']].copy()\n",
    "top_artists_by_churn = (\n",
    "    artist_stat_large_df.groupby('artist', dropna=False)['churn']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    ")\n",
    "top_artists_by_churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e03d6-369a-4cd0-a743-77e4bdfee29f",
   "metadata": {},
   "source": [
    "#### C, Calculate average churn rate by level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775b223-776e-45b5-ac05-9a64d0d77ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_stat_large_df = df_large_dataset_churn_processed_non_missing_non_duplicated[['level', 'churn']].copy()\n",
    "level_stat_large_df['churn'] = pd.to_numeric(level_stat_large_df['churn'], errors='coerce')\n",
    "churn_by_level = level_stat_large_df.groupby('level')['churn'].mean() * 100\n",
    "\n",
    "print('Proportion of users that churned from free subscription:', churn_by_level.get('free', 'N/A'))\n",
    "print('Proportion of users that churned from paid subscription:', churn_by_level.get('paid', 'N/A'))\n",
    "\n",
    "churn_by_level.plot(kind='bar')\n",
    "plt.ylabel('Average Churn Rate (%)')\n",
    "plt.title('Average Churn Rate by By Subcription Type')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558e59-af63-4789-a1d0-7fb5fe8de6a4",
   "metadata": {},
   "source": [
    "#### D, Number of churns per State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4730cb5-7168-48b3-bb57-a42150a2d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_stat_large_df = df_large_dataset_churn_processed_non_missing_non_duplicated[['location', 'churn']].copy()\n",
    "location_stat_large_df['churn'] = pd.to_numeric(location_stat_large_df['churn'], errors='coerce')\n",
    "location_stat_large_df['state'] = location_stat_large_df['location'].str.split(',').str[1].str.strip()\n",
    "state_stat_large_df = location_stat_large_df.copy()\n",
    "state_stat_large_df.drop(columns='location', inplace=True)\n",
    "state_churn = state_stat_large_df.groupby('state')['churn'].sum().reset_index()\n",
    "state_churn = state_churn[state_churn['churn'] > 0]\n",
    "top_states = state_churn.sort_values(by='churn', ascending=False).reset_index().drop(columns='index').head(10)\n",
    "\n",
    "print('Viewing top 10 states with churn:\\n')\n",
    "print(top_states)\n",
    "\n",
    "top_states.plot(kind='bar', x='state', y='churn', legend=False)\n",
    "plt.ylabel('Total Churn Count')\n",
    "plt.title('Top 10 States by Total Churn')\n",
    "plt.xticks(rotation=360)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe7e86-3a48-4a37-93de-34c08320367d",
   "metadata": {},
   "source": [
    "#### E, Time-Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8dba7-787b-492f-b6e4-b0227626699f",
   "metadata": {},
   "source": [
    "##### Convert 'ts' from milliseconds to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5ca65-807c-4330-9432-dc6ad89bfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df_processed_time = df_large_dataset_churn_processed_non_missing_non_duplicated.copy()\n",
    "large_df_processed_time['ts'] = pd.to_datetime(large_df_processed_time['ts'], unit='ms')\n",
    "\n",
    "large_df_processed_time['hour'] = large_df_processed_time['ts'].dt.hour\n",
    "large_df_processed_time['day'] = large_df_processed_time['ts'].dt.day\n",
    "large_df_processed_time['month'] = large_df_processed_time['ts'].dt.month\n",
    "large_df_processed_time['week_day'] = large_df_processed_time['ts'].dt.weekday  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85d65f-7840-4f75-9cbc-087b12140fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_churn_distribution_by_column(column_name, churn_label, normalize=False):\n",
    "    filtered_df = large_df_processed_time[large_df_processed_time['churn'] == churn_label]\n",
    "    churn_distribution = filtered_df.groupby(column_name)['userId'].count()\n",
    "    try:\n",
    "        churn_distribution.index = churn_distribution.index.astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    if normalize:\n",
    "        churn_distribution = churn_distribution / churn_distribution.sum() * 100\n",
    "    return churn_distribution.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5e22f-c6f4-41a7-af00-5cdb62d509f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_churn_distribution_by_time(time_column, normalize=True, figsize=(16, 4), chart_title=None, label_rotation=0):\n",
    "    churn_time_df = pd.DataFrame({\n",
    "        'Churned Users': get_churn_distribution_by_column(time_column, churn_label=1, normalize=normalize),\n",
    "        'Active Users': get_churn_distribution_by_column(time_column, churn_label=0, normalize=normalize)\n",
    "    })\n",
    "    ax = churn_time_df.plot(kind='bar', figsize=figsize)\n",
    "    if chart_title is None:\n",
    "        chart_title = time_column\n",
    "    ax.set_ylabel('Percentage of Users' if normalize else 'User Count')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=label_rotation)\n",
    "    ax.set_title(f'{\"Percentage\" if normalize else \"Count\"} of Users by {chart_title}')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069cf20-f191-48d2-9c77-f25646dd9a24",
   "metadata": {},
   "source": [
    "##### i, Percentage of Users Churn by Hour of the Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d662e87-e57b-469c-b0d4-d33d00b5a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn_distribution_by_time('hour', chart_title='Hour of the Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b98fdb-f522-4515-9ed6-9773851459c9",
   "metadata": {},
   "source": [
    "##### ii, Percentage of Users Churn by Day in a Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58325fd-35fb-4366-b1ed-065ab0af042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn_distribution_by_time('day', chart_title='Day In A Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e3dd8-0bbf-4af3-9d29-c65089967945",
   "metadata": {},
   "source": [
    "##### iii, Percentage of Users Churn by Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e6cd0-8285-411e-949f-e0d79b6f8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn_distribution_by_time('week_day', chart_title='Weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4ae62-0791-4595-b48e-ec8098173401",
   "metadata": {},
   "source": [
    "##### iv, Percentage of Users Churn by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bac8dc-9972-4594-bc3d-934d14a57805",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_churn_distribution_by_time('month', chart_title='Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce0590-9ab9-4183-944e-fb2f02f76dd2",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1945e-1643-45f2-91b8-3a5d594f63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_large_df = large_df_processed_time.copy()\n",
    "feature_engineering_large_df['churn'] = pd.to_numeric(feature_engineering_large_df['churn'], errors='coerce')\n",
    "feature_engineering_large_df['state'] = feature_engineering_large_df['location'].str.split(',').str[1].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e33b1-9bbc-4712-b376-b20c98deca9d",
   "metadata": {},
   "source": [
    "##### i. Label encode ordinal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dd517-69ea-451e-a6a7-e32cff3a055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = ['level']\n",
    "le = LabelEncoder()\n",
    "for col in ordinal_cols:\n",
    "    feature_engineering_large_df[col] = le.fit_transform(feature_engineering_large_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cd978-164e-4b95-9a0e-e32e8ca9a3de",
   "metadata": {},
   "source": [
    "##### ii. One-hot encode nominal categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a35b83-c0df-4f4f-a984-ec5c8469c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols = [\n",
    "    'page', 'auth', 'method', 'location', 'gender', 'state'\n",
    "]\n",
    "\n",
    "nominal_backup = feature_engineering_large_df[nominal_cols].copy()\n",
    "feature_engineering_large_df[nominal_cols] = feature_engineering_large_df[nominal_cols].astype(str)\n",
    "feature_engineering_large_df = pd.get_dummies(feature_engineering_large_df, columns=nominal_cols, drop_first=False)\n",
    "feature_engineering_large_df = pd.concat([feature_engineering_large_df, nominal_backup], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdd968-f97f-4fbf-ba25-ebf892cfbd9b",
   "metadata": {},
   "source": [
    "##### iii. Calculate the length of object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60647037-1392-49b9-a186-258f881d486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_large_df = ['userAgent', 'lastName', 'firstName', 'artist', 'song']\n",
    "\n",
    "for col in object_columns_large_df:\n",
    "    feature_engineering_large_df[f\"value_length_{col}\"] = feature_engineering_large_df[col].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bddca0-f488-42a0-aeb9-b2ba2874ffbf",
   "metadata": {},
   "source": [
    "#### A, Feature LifeTime Since Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be59e9-9be4-4140-a3c3-955954967cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_large_df['registration'] = pd.to_datetime(feature_engineering_large_df['registration'], unit='ms')\n",
    "feature_engineering_large_df['lifetime'] = (feature_engineering_large_df['ts'] - feature_engineering_large_df['registration']).dt.total_seconds()\n",
    "\n",
    "feature_1 = feature_engineering_large_df.groupby('userId')['lifetime'].max().reset_index()\n",
    "feature_1['lifetime'] = feature_1['lifetime'] / (3600 * 24)\n",
    "\n",
    "print(feature_1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643b120-41f3-49e3-b52b-adaab8332557",
   "metadata": {},
   "source": [
    "#### B, Feature Total Songs Listened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233dc1e-0667-469e-a4cd-bcd7a9e36a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 = feature_engineering_large_df.groupby('userId')['song'].count().reset_index()\n",
    "feature_2 = feature_2.rename(columns={'song': 'total_songs'})\n",
    "\n",
    "print(feature_2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba20af-ae51-445d-bf4a-768289a6727b",
   "metadata": {},
   "source": [
    "#### C, Feature Total Songs Liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4702db-7abb-4c61-9553-deb1695d89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3 = feature_engineering_large_df[feature_engineering_large_df['page'] == 'Thumbs Up'].groupby('userId')['page'].count().reset_index()\n",
    "feature_3 = feature_3.rename(columns={'page': 'num_thumb_up'})\n",
    "\n",
    "print(feature_3.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf69bd-c5cf-43d7-9e49-41d19fd49e4a",
   "metadata": {},
   "source": [
    "#### D, Feature Total Songs Disliked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6a925-1269-425b-adc6-8e5c0038da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_4 = feature_engineering_large_df[feature_engineering_large_df['page'] == 'Thumbs Down'].groupby('userId')['page'].count().reset_index()\n",
    "feature_4 = feature_4.rename(columns={'page': 'num_thumb_down'})\n",
    "\n",
    "print(feature_4.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed8306-93a7-4d1c-955d-d2c2e7f16298",
   "metadata": {},
   "source": [
    "#### E, Feature Playlist length: (Check for all the Add to Playlist Page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806a5f4-e9ea-42b5-bc6b-50b31ff190a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 = feature_engineering_large_df[feature_engineering_large_df['page'] == 'Add to Playlist'].groupby('userId')['page'].count().reset_index()\n",
    "feature_5 = feature_5.rename(columns={'page': 'add_to_playlist'})\n",
    "\n",
    "print(feature_5.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7d9a0-98b7-4a4f-a129-8a04e36e4331",
   "metadata": {},
   "source": [
    "#### F, Feature Referring friends: (Check for All the Add Friend Page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83c119-2275-4d80-a0a6-2c90d7d031be",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_6 = feature_engineering_large_df[feature_engineering_large_df['page'] == 'Add Friend'].groupby('userId')['page'].count().reset_index()\n",
    "feature_6 = feature_6.rename(columns={'page': 'add_friend'})\n",
    "\n",
    "print(feature_6.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d779322-98db-48b6-9769-2c51b1626d31",
   "metadata": {},
   "source": [
    "#### G, Feature Listening Longevity: ( Check for All the total listen time each user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7d2f9-c1cd-4266-8b8a-a52657130ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_7 = feature_engineering_large_df.groupby('userId')['length'].sum().reset_index()\n",
    "feature_7 = feature_7.rename(columns={'length': 'listen_time'})\n",
    "\n",
    "print(feature_7.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ac211-7f8a-4b9d-b9a9-b8ea08d1964a",
   "metadata": {},
   "source": [
    "#### H, Feature Songs per Session: (Avange song played per Sessions Count the number user hit NextSong group by sessionId, userId and take avarange of the number of song be played corresponding to that SessionID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be666629-1fd2-4abe-9172-a794a5068ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_song_df = feature_engineering_large_df[feature_engineering_large_df['page'] == 'NextSong']\n",
    "songs_per_session = next_song_df.groupby(['userId', 'sessionId']).size().reset_index(name='song_count')\n",
    "\n",
    "feature_8 = songs_per_session.groupby('userId')['song_count'].mean().reset_index()\n",
    "feature_8 = feature_8.rename(columns={'song_count': 'avg_songs_played'})\n",
    "\n",
    "print(feature_8.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d0928-b902-4b7d-8ba1-36e222ad63c6",
   "metadata": {},
   "source": [
    "#### I, Feature Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7388c71-50aa-45bf-9a70-229c1edef7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_9 = feature_engineering_large_df[['userId', 'gender_F']].drop_duplicates()\n",
    "feature_9 = feature_9.rename(columns={'gender_F': 'gender'})\n",
    "\n",
    "feature_9['gender'] = feature_9['gender'].astype(int)\n",
    "\n",
    "print(feature_9.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffc5d2-29a7-4a39-b91f-522931529284",
   "metadata": {},
   "source": [
    "#### K, Feature Number of Artists Listened: ( Count the total of Artists that each user listen to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2591ddb-bedc-4312-8fbe-e2ed58783edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_song_df = feature_engineering_large_df[feature_engineering_large_df['page'] == 'NextSong']\n",
    "\n",
    "artist_per_session = (\n",
    "    next_song_df.groupby(['userId', 'sessionId'])['artist']\n",
    "    .nunique()\n",
    "    .reset_index(name='unique_artist_count')\n",
    ")\n",
    "\n",
    "feature_10 = (\n",
    "    artist_per_session.groupby('userId')['unique_artist_count']\n",
    "    .mean()\n",
    "    .reset_index(name='avg_unique_artists_per_session')\n",
    ")\n",
    "\n",
    "print(feature_10.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef934e09-19ad-48b2-83f4-c2729f2121e7",
   "metadata": {},
   "source": [
    "#### L, Feature Number Time User Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e10ae3-7a57-483a-a4a4-7df12c62438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_11 = (\n",
    "    feature_engineering_large_df[feature_engineering_large_df['page'] == 'Logged In']\n",
    "    .groupby('userId')['page']\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={'page': 'login_count'})\n",
    ")\n",
    "\n",
    "print(feature_11.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfc816-9996-4097-82aa-556a215da5c2",
   "metadata": {},
   "source": [
    "#### M Feature Number Time User Logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4a197-5547-413e-8d73-1aa7776265b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_12 = (\n",
    "    feature_engineering_large_df[feature_engineering_large_df['page'] == 'Logged Out']\n",
    "    .groupby('userId')['page']\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={'page': 'logout_count'})\n",
    ")\n",
    "\n",
    "print(feature_13.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0053e5b-b08b-4712-8e13-858b0b5cb50b",
   "metadata": {},
   "source": [
    "#### N, Feature Number Time User Downgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b3fd4-41df-49d2-be82-7727c904e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_13 = (\n",
    "    feature_engineering_large_df[feature_engineering_large_df['page'] == 'Downgrade']\n",
    "    .groupby('userId')['page']\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={'page': 'downgrade_count'})\n",
    ")\n",
    "print(feature_14.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af62015-9ae5-4a8c-9d01-2766d1a31e3f",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46697dd6-fa18-469d-b7d2-4127a38774f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_churn = feature_engineering_large_df[['userId', 'churn']].drop_duplicates().rename(columns={'churn': 'label'})\n",
    "print(label_churn.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab30138-98ca-4545-92a5-72f4200c9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = feature_1 \\\n",
    "    .merge(feature_2, on='userId', how='outer') \\\n",
    "    .merge(feature_3, on='userId', how='outer') \\\n",
    "    .merge(feature_4, on='userId', how='outer') \\\n",
    "    .merge(feature_5, on='userId', how='outer') \\\n",
    "    .merge(feature_6, on='userId', how='outer') \\\n",
    "    .merge(feature_7, on='userId', how='outer') \\\n",
    "    .merge(feature_8, on='userId', how='outer') \\\n",
    "    .merge(feature_9, on='userId', how='outer') \\\n",
    "    .merge(feature_10, on='userId', how='outer') \\\n",
    "    .merge(feature_11, on='userId', how='outer') \\\n",
    "    .merge(feature_12, on='userId', how='outer') \\\n",
    "    .merge(feature_13, on='userId', how='outer') \\\n",
    "    .merge(label_churn, on='userId', how='outer') \\\n",
    "    .drop(columns='userId').fillna(0)\n",
    "\n",
    "print(merged_data.describe())\n",
    "print(merged_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418d0f4-54e5-4787-9485-0a3b75544cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = merged_data.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', center=0, square=True, linewidths=0.5)\n",
    "\n",
    "plt.title('Correlation Heatmap of All Features (Including Label)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
